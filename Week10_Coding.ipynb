{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86c7c92-f5da-4a89-9aa7-4f3e06f85be7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b273c10a-aad2-43ab-8e21-749460bf3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# --- Standard Library ---\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# --- Third-Party Libraries ---\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- TensorFlow / Keras ---\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Sequential, layers, models\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay, ExponentialDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, GlobalAveragePooling1D, Dense, \n",
    "    LSTM, GRU, Dropout, SpatialDropout1D, Bidirectional, Lambda\n",
    ")\n",
    "\n",
    "# Preprocessing\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "os.environ[\"TRANSFORMERS_NO_TORCH\"] = \"1\"   # <- correct flag name for recent versions\n",
    "\n",
    "\n",
    "# --- Reproducibility Settings ---\n",
    "random_seed = 42\n",
    "\n",
    "'''\n",
    "# OS-level determinism\n",
    "os.environ['PYTHONHASHSEED'] = '0'        # Disable hash randomization\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'  # Make TF ops deterministic (where possible)\n",
    "os.environ['TF_CUDNN_DETERMINISM'] = '1'  # CuDNN deterministic (if using GPU)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow INFO and WARNING messages\n",
    "\n",
    "# Set seeds\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "'''\n",
    "\n",
    "# --- Utility Function ---\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n",
    "'''\n",
    "# Example usage for timing code:\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# <your code here>\n",
    "\n",
    "print(\"Execution Time:\", format_hms(time.time() - start_time))\n",
    "'''\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c168283c-85fe-4258-af24-7ef5b8dd5b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed (comment out if your env already has these)\n",
    "# Note: 'sentencepiece' is required for T5 tokenization.\n",
    "# %pip install -q transformers torch sentencepiece accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6555ec89-6d62-4af9-b437-84d2e8674355",
   "metadata": {},
   "source": [
    "### BERT-style masked language modeling (bidirectional)\n",
    "\n",
    "Teaching notes:\n",
    "\n",
    "BERT uses masked language modeling (bidirectional context).\n",
    "\n",
    "Different pretraining objective vs GPT → different strengths (representation learning, classification, QA heads, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfee2ee-6ef4-4bff-9ffa-ba036bd0478a",
   "metadata": {},
   "source": [
    "### Bert Classifies the IMDB Movie Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e0f3a1-ae85-48f9-94e0-823d6cf555f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers datasets accelerate scikit-learn\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# 1) Data\n",
    "ds = load_dataset(\"imdb\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tok(ex):\n",
    "    return tokenizer(ex[\"text\"], truncation=True, max_length=256)\n",
    "ds_tok = ds.map(tok, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# 2) Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 3) Metrics\n",
    "def metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds), \"f1\": f1_score(labels, preds)}\n",
    "\n",
    "# 4) Train\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"imdb-distilbert\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,              # 2–3 is usually enough\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_tok[\"train\"],\n",
    "    eval_dataset=ds_tok[\"test\"].shuffle(seed=42).select(range(5000)),  # speed-up eval\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate(ds_tok[\"test\"])     # final full test evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f4b66-9814-48e2-9338-78eb32b507e1",
   "metadata": {},
   "source": [
    "### Bert Classifies Sentences a Sequential or Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3950fd5-a225-4002-98e9-2b81c43c52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install transformers datasets nltk accelerate scikit-learn\n",
    "\n",
    "import random, nltk, numpy as np\n",
    "nltk.download('punkt')\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 1) Build sentence pairs from raw documents\n",
    "docs = [\n",
    "    # Put any longer texts here (articles, chapters). Add multiple docs for variety.\n",
    "    \"\"\"Natural language processing is widely used in industry. It powers search, chatbots, and content moderation. \n",
    "    Modern models rely on large-scale pretraining. Fine-tuning adapts them to specific tasks.\"\"\",\n",
    "    \"\"\"Decision trees split data using impurity metrics. Random forests reduce variance by bagging multiple trees.\n",
    "    Boosting methods like XGBoost focus on hard-to-predict examples. Regularization helps avoid overfitting.\"\"\"\n",
    "]\n",
    "\n",
    "sentences = []\n",
    "for d in docs:\n",
    "    sentences += nltk.sent_tokenize(d)\n",
    "\n",
    "pairs, labels = [], []\n",
    "# Positive pairs: consecutive sentences\n",
    "for i in range(len(sentences)-1):\n",
    "    pairs.append((sentences[i], sentences[i+1]))\n",
    "    labels.append(1)\n",
    "\n",
    "# Negative pairs: random mismatches (same count as positives)\n",
    "for i in range(len(sentences)-1):\n",
    "    j = random.randrange(len(sentences))\n",
    "    while j == i+1:  # ensure not the true next sentence\n",
    "        j = random.randrange(len(sentences))\n",
    "    pairs.append((sentences[i], sentences[j]))\n",
    "    labels.append(0)\n",
    "\n",
    "# 2) Hugging Face dataset\n",
    "data = {\"s1\": [p[0] for p in pairs], \"s2\": [p[1] for p in pairs], \"label\": labels}\n",
    "ds = Dataset.from_dict(data).train_test_split(test_size=0.25, seed=42)\n",
    "\n",
    "# 3) Tokenize as sentence pairs\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tok(batch):\n",
    "    return tokenizer(batch[\"s1\"], batch[\"s2\"], truncation=True, max_length=256)\n",
    "\n",
    "ds_tok = ds.map(tok, batched=True, remove_columns=[\"s1\",\"s2\"])\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 4) Model + metrics\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "def metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds), \"f1\": f1_score(labels, preds)}\n",
    "\n",
    "# 5) Train\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"sent-follow-distilbert\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_tok[\"train\"],\n",
    "    eval_dataset=ds_tok[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d3f1bf3-121d-434c-918d-47a70a8f50be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_PYTORCH\"] = \"1\"   # make Transformers ignore PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08d129ef-e485-4332-858e-fc523b095e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: 2.17.0\n",
      "tf_keras importable? True\n",
      "Transformers: 4.44.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf, importlib, transformers\n",
    "print(\"TF:\", tf.__version__)  # should be 2.17.0\n",
    "print(\"tf_keras importable?\", importlib.util.find_spec(\"tf_keras\") is not None)  # True\n",
    "print(\"Transformers:\", transformers.__version__)  # 4.44.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bca9bf36-1fb0-4584-ab80-33673940ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e98d183-f063-47b8-a8e8-4014c2057b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.4167880415916443, 'token': 3000, 'token_str': 'paris', 'sequence': 'the capital of france is paris.'}, {'score': 0.07141624391078949, 'token': 22479, 'token_str': 'lille', 'sequence': 'the capital of france is lille.'}, {'score': 0.06339256465435028, 'token': 10241, 'token_str': 'lyon', 'sequence': 'the capital of france is lyon.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "fill = pipeline(\"fill-mask\", model=\"bert-base-uncased\", framework=\"tf\")  # or distilbert\n",
    "print(fill(\"The capital of France is [MASK].\", top_k=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dabbeebe-37d5-418d-ab80-6133f4653dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three economists shared the prize for research on the economics of capitalism at the Oxford Business School: \"The authors' findings demonstrate that capitalism is not an irrational pursuit of individual value and therefore should not be pursued by government and private individuals.\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "gen = pipeline(\"text-generation\", model=\"distilgpt2\", framework=\"tf\")\n",
    "print(gen(\"The three economists shared the prize for research\",\n",
    "          max_new_tokens=100, do_sample=True, temperature=0.8)[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41dc9b45-9542-413a-b585-1b300f530ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three economists shared the prize for research-supported research in recent years. He is now a Harvard News Fellow.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "gen = pipeline(\"text-generation\", model=\"distilgpt2\", framework=\"tf\")\n",
    "print(gen(\"The three economists shared the prize for research\",\n",
    "          max_new_tokens=100, do_sample=True, temperature=0.8)[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2700ce8-3ba3-4af1-a650-f299081e0990",
   "metadata": {},
   "source": [
    "### Baseline T5-Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c28e85c0-37a2-4f39-9a1a-8021f0740c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an article from NYTs:  https://www.nytimes.com/2025/10/13/business/nobel-prize-economics.html\n",
    "\n",
    "text = \"The Nobel Memorial Prize in Economic Sciences was awarded on Monday to Joel Mokyr of Northwestern University, Philippe Aghion of Collège de France, INSEAD and the London School of Economics and Peter Howitt of Brown University for their work on how innovation drives economic growth. \\\n",
    "The three economists shared the prize for research that explains the relationship between technological progress and sustained economic growth that has improved living standards, health and quality of life for people around the world. The prize committee said that their work would help ensure that growth was maintained and could be steered in the direction to support humankind. \\\n",
    "For most of human history, there was very little economic growth, John Hassler, the chair of the prize committee, said in a ceremony announcing the award. Despite important discoveries that improved living conditions, growth leveled off. But over the past two centuries, that changed. 'Sustained economic growth, driven by a continuous stream of technological innovations and improvements, has replaced stagnation,' Mr. Hassler said. \\\n",
    "The award for the economists’ work comes as artificial intelligence has become an increasingly dominant force in the global economy and has the potential to spur another technology-driven boom. But other policies are expected to restrain economic growth, such as the Trump administration’s tariffs and other protectionist policies, like China’s curbs on exports of rare earth minerals and battery-making equipment. \\\n",
    "Mr. Mokyr was awarded half the prize for his work in explaining how sustained economic growth became the norm. \\\n",
    "He showed that for innovations to succeed and become a self-generating process, people needed a scientific explanation for why the breakthroughs worked. Before the industrial revolution, a lack of this knowledge made it difficult to build on new discoveries, the committee said. \\\n",
    "Mr. Mokyr’s work, such as his book 'A Culture of Growth: Origins of the Modern Economy,' has also emphasized the importance of society being open to new ideas and allowing change. Mr. Mokyr has tended to take an optimistic view on the potential for more economic growth. \\\n",
    "Mr. Aghion and Mr. Howitt shared the other half of the award for what the committee described as 'the theory of sustained growth through creative destruction.' They built a mathematical model for growth, with creative destruction as a core element. \\\n",
    "The committee described creative destruction as 'an endless process in which new and better products replace the old.' They used the example of the telephone, in which each new version made the previous one obsolete, from the rotary dial phone in the early 1900s through to today’s smartphones. \\\n",
    "Mr. Aghion and Mr. Howitt’s work shows how economic growth can continue despite companies being sidelined by the innovation of other firms. Their work can support policymakers in designing research and development policies, the committee said. \\\n",
    "The laureates’ work shows 'we should not take progress for granted,' Kerstin Enflo, a member of the Nobel committee, said during a news conference. \\\n",
    "'Instead, society must keep an eye on the factors that generate and sustain economic growth,' she added. 'These are science-based innovation, creative destruction and a society open for change.' \\\n",
    "Mr. Aghion said that the prize came as 'a huge surprise.' \\\n",
    "'I can’t find the words to express what I feel,' he added by telephone at the news conference. He also warned against forces like protectionism and tariffs that obstruct growth and are gaining traction. \\\n",
    "Trade barriers and deglobalization make markets more fragmented and reduce opportunities to exchange ideas, he said.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04d68471-d3b1-4248-9d6b-5e8c628749e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the three economists shared the prize for research that explains the relationship between technological progress and sustained economic growth that has improved living standards, health and quality of life for people around the world . the prize committee said that their work would ensure that growth was maintained and could be steered in the direction to support humankind .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summ = pipeline(\"summarization\", model=\"t5-small\", framework=\"tf\")\n",
    "print(summ(text, max_length=100, min_length=10)[0][\"summary_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714cf3e-47e5-4065-bb0b-0b8fd82e68f3",
   "metadata": {},
   "source": [
    "## 1) Decoding knobs (quality vs speed)\n",
    "\n",
    "\n",
    "See the **Appendix** for more details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7efb6cd-1117-4c83-8db4-5cb56391125d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "163a1ddd-e89d-4777-8d6f-347cdbc2e3d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the three economists shared the prize for research that explains the relationship between technological progress and sustained economic growth that has improved living standards, health and quality of life for people around the world . the prize committee said that their work would ensure that growth was maintained and could be steered in the direction to support humankind .\n",
      "the three economists shared the prize for research that explains the relationship between technological progress and sustained economic growth that has improved living standards, health and quality of life for people around the world . the prize committee said that their work would ensure that growth was maintained and could be steered in the direction to support humankind .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# summ = pipeline(\"summarization\", model=\"t5-small\", framework=\"tf\")\n",
    "\n",
    "# Beam search (deterministic, usually better)\n",
    "print(summ(text, max_length=120, min_length=40,\n",
    "           num_beams=4, length_penalty=1.0, no_repeat_ngram_size=3)[0][\"summary_text\"])\n",
    "\n",
    "# Sampling (more diverse; can drift)\n",
    "print(summ(text, max_length=120, min_length=40,\n",
    "           do_sample=True, top_p=0.9, top_k=50, temperature=0.8,\n",
    "           no_repeat_ngram_size=3)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2783a4bd-37ae-4293-ba76-ef105deb8f9c",
   "metadata": {},
   "source": [
    "## 2) Style controls (T5 follows instructions well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1bb9b-5c75-4aef-bebe-6cfda5b647c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5(prompt): \n",
    "    return summ(f\"{prompt} {text}\", max_length=80, min_length=20, num_beams=4)[0][\"summary_text\"]\n",
    "\n",
    "print(t5(\"summarize:\"))                          # default\n",
    "print(t5(\"summarize in 1 sentence:\"))\n",
    "print(t5(\"summarize in 3 bullet points:\"))       # will often produce bullets\n",
    "print(t5(\"write a headline:\"))\n",
    "print(t5(\"summarize for a 9th-grade reader:\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538fb42-a460-420c-b189-d74e4366a7b5",
   "metadata": {},
   "source": [
    "## 3) Headline + bullets in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c414da0a-7799-4f40-be6c-d2f3f8e16cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = summ(\"write a headline: \" + text, max_length=20, num_beams=4)[0][\"summary_text\"]\n",
    "bullets  = summ(\"summarize in 3 bullet points: \" + text, max_length=90, num_beams=4)[0][\"summary_text\"]\n",
    "print(headline, \"\\n\", bullets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b71aa8-af9a-4e94-8a08-ba334bf071fd",
   "metadata": {},
   "source": [
    "## 4) Long articles (chunk + combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccfd5e5-9e55-4166-848e-1a8ae433cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(xs, n=512):  # T5-small context is ~512 tokens; rough char fallback\n",
    "    step = 1500\n",
    "    return [xs[i:i+step] for i in range(0, len(xs), step)]\n",
    "\n",
    "partials = [summ(\"summarize: \" + c, max_length=80, num_beams=4)[0][\"summary_text\"]\n",
    "            for c in chunk(text)]\n",
    "final = summ(\"summarize: \" + \" \".join(partials), max_length=120, num_beams=4)[0][\"summary_text\"]\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef8fac5-bad9-482a-a13f-cb2db842b24b",
   "metadata": {},
   "source": [
    "## 5) Model swaps (quick comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ead69-2d13-4713-a332-ac7e804be9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster/smarter variants to try:\n",
    "models = [\n",
    "  \"t5-small\",                 # baseline\n",
    "  \"google/flan-t5-small\",     # better instruction-following\n",
    "  \"facebook/bart-large-cnn\",  # strong for news\n",
    "  \"sshleifer/distilbart-cnn-12-6\"  # faster BART\n",
    "]\n",
    "for m in models:\n",
    "    p = pipeline(\"summarization\", model=m, framework=\"tf\")\n",
    "    print(m, \"→\", p(text, max_length=120, min_length=40, num_beams=4)[0][\"summary_text\"][:160], \"…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54396eb4-dd4e-453a-962e-403dd952ec48",
   "metadata": {},
   "source": [
    "*(For very long docs, look at `allenai/led-base-16384` or `google/pegasus-cnn_dailymail`.)*\n",
    "\n",
    "## 6) Quick sanity checks (length, redundancy, faithfulness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e796b8ef-b334-491c-9691-4f836a2902f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = summ(text, max_length=120, min_length=40, num_beams=4,\n",
    "         no_repeat_ngram_size=3)[0][\"summary_text\"]\n",
    "print(\"chars:\", len(s), \"| sentences:\", s.count(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3974cb9c-dbc2-4913-8041-01d7383ac14b",
   "metadata": {},
   "source": [
    "Optional: compare two settings with a lightweight metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a5c5b-f4ec-4c09-b5d8-672628329b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install rouge-score\n",
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rougeL\"], use_stemmer=True)\n",
    "r = scorer.score(text, s)\n",
    "print({k: v.fmeasure for k,v in r.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5209e56-b047-45be-82ca-4f8dcb9493b3",
   "metadata": {},
   "source": [
    "## Appendix: Beam Search and Sampling in T5\n",
    "\n",
    "\n",
    "Here’s what each argument is doing and how to pick values—first for beam search (deterministic), then for sampling (stochastic).\n",
    "\n",
    "# Shared length controls\n",
    "\n",
    "* `max_length=120`: Hard cap on output length in **tokenizer tokens**. Pick a ceiling that fits your UI or rubric; smaller = faster, safer; larger = risk of drift.\n",
    "* `min_length=40`: Forces at least this many tokens before the model is allowed to stop. Use to avoid ultra-short summaries (raise if you see 1–2 sentence outputs).\n",
    "\n",
    "# Anti-repetition\n",
    "\n",
    "* `no_repeat_ngram_size=3`: Disallows any 3-gram from appearing twice. 2–4 are common; higher reduces loops but can overconstrain phrasing.\n",
    "\n",
    "# Beam search block\n",
    "\n",
    "* `num_beams=4`: How many hypotheses are explored in parallel. More beams (e.g., 6–8) → usually better coverage/faithfulness, but slower and sometimes more generic.\n",
    "* `length_penalty=1.0`: Normalizes beams by length.\n",
    "\n",
    "  * `>1.0` penalizes long outputs → shorter, punchier.\n",
    "  * `<1.0` rewards length → longer, more detailed.\n",
    "  * Start at `1.0`; try `1.1–1.2` if outputs ramble; try `0.9` if they’re too terse.\n",
    "\n",
    "# Sampling block\n",
    "\n",
    "* `do_sample=True`: Switches from greedy/beam decoding to probabilistic sampling.\n",
    "* `temperature=0.8`: Scales logits before sampling.\n",
    "\n",
    "  * `<1` (e.g., 0.7–0.9) = safer, crisper.\n",
    "  * `>1` (e.g., 1.1–1.3) = more varied, more risk.\n",
    "* `top_p=0.9` (nucleus sampling): Sample only from the smallest set of tokens whose cumulative probability ≥ 0.9. Lower to 0.8 to be safer; raise to 0.95 for more variety.\n",
    "* `top_k=50`: Also restrict to the top-50 tokens by probability. When both `top_p` and `top_k` are set, the candidate set is the intersection—this adds an extra safety brake. Typical `top_k` range: 40–100.\n",
    "\n",
    "# Quick recipes\n",
    "\n",
    "* **Max faithfulness / less drift (deterministic):** `num_beams=6–8`, `length_penalty=1.0–1.2`, keep `no_repeat_ngram_size=3`, tune `min_length`.\n",
    "* **Balanced variety (sampling):** `temperature=0.8–0.9`, `top_p=0.9`, `top_k=50–80`, keep `no_repeat_ngram_size=3`.\n",
    "* **More creative / riskier:** `temperature≈1.0–1.2`, `top_p=0.92–0.95`, `top_k=0 or 100+` (or drop `top_k`), possibly lower `min_length`.\n",
    "\n",
    "# Troubleshooting heuristics\n",
    "\n",
    "* Too short → raise `min_length` or lower `length_penalty`.\n",
    "* Repetitive → increase `no_repeat_ngram_size` or lower `top_p`/`temperature`.\n",
    "* Generic/boring (beam) → try sampling block or reduce `num_beams`.\n",
    "* Off-topic (sampling) → lower `temperature`, lower `top_p`, keep `top_k` moderate.\n",
    "\n",
    "That’s it—tune length first, then repetition, then exploration (beams or sampling) depending on faithfulness vs. variety.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35188cba-0496-4d92-8e8d-253b6159588e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5ff163-a66a-49aa-afdc-58fdc0db251b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3be4253-184a-4b0e-8546-1f3954cd51da",
   "metadata": {},
   "source": [
    "* `num_beams`: 1–6 (higher = slower, usually better)\n",
    "* `no_repeat_ngram_size`: 2–4 to curb repetition\n",
    "* `length_penalty`: >1 favors longer, <1 favors shorter\n",
    "* `do_sample` + `top_p`/`top_k`/`temperature`: enable stochastic summaries\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Style controls (T5 follows instructions well)\n",
    "\n",
    "```python\n",
    "def t5(prompt): return summ(f\"{prompt} {text}\", max_length=80, min_length=20, num_beams=4)[0][\"summary_text\"]\n",
    "\n",
    "print(t5(\"summarize:\"))                          # default\n",
    "print(t5(\"summarize in 1 sentence:\"))\n",
    "print(t5(\"summarize in 3 bullet points:\"))       # will often produce bullets\n",
    "print(t5(\"write a headline:\"))\n",
    "print(t5(\"summarize for a 9th-grade reader:\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Headline + bullets in one go\n",
    "\n",
    "```python\n",
    "headline = summ(\"write a headline: \" + text, max_length=20, num_beams=4)[0][\"summary_text\"]\n",
    "bullets  = summ(\"summarize in 3 bullet points: \" + text, max_length=90, num_beams=4)[0][\"summary_text\"]\n",
    "print(headline, \"\\n\", bullets)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Long articles (chunk + combine)\n",
    "\n",
    "```python\n",
    "def chunk(xs, n=512):  # T5-small context is ~512 tokens; rough char fallback\n",
    "    step = 1500\n",
    "    return [xs[i:i+step] for i in range(0, len(xs), step)]\n",
    "\n",
    "partials = [summ(\"summarize: \" + c, max_length=80, num_beams=4)[0][\"summary_text\"]\n",
    "            for c in chunk(text)]\n",
    "final = summ(\"summarize: \" + \" \".join(partials), max_length=120, num_beams=4)[0][\"summary_text\"]\n",
    "print(final)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Model swaps (quick comparisons)\n",
    "\n",
    "```python\n",
    "# Faster/smarter variants to try:\n",
    "models = [\n",
    "  \"t5-small\",                 # baseline\n",
    "  \"google/flan-t5-small\",     # better instruction-following\n",
    "  \"facebook/bart-large-cnn\",  # strong for news\n",
    "  \"sshleifer/distilbart-cnn-12-6\"  # faster BART\n",
    "]\n",
    "for m in models:\n",
    "    p = pipeline(\"summarization\", model=m, framework=\"tf\")\n",
    "    print(m, \"→\", p(text, max_length=120, min_length=40, num_beams=4)[0][\"summary_text\"][:160], \"…\")\n",
    "```\n",
    "\n",
    "*(For very long docs, look at `allenai/led-base-16384` or `google/pegasus-cnn_dailymail`.)*\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Quick sanity checks (length, redundancy, faithfulness)\n",
    "\n",
    "```python\n",
    "s = summ(text, max_length=120, min_length=40, num_beams=4,\n",
    "         no_repeat_ngram_size=3)[0][\"summary_text\"]\n",
    "print(\"chars:\", len(s), \"| sentences:\", s.count(\".\"))\n",
    "```\n",
    "\n",
    "Optional: compare two settings with a lightweight metric:\n",
    "\n",
    "```python\n",
    "# pip install rouge-score\n",
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rougeL\"], use_stemmer=True)\n",
    "r = scorer.score(text, s)\n",
    "print({k: v.fmeasure for k,v in r.items()})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Want me to wrap these into a neat “Summarization Lab” section for your transformer notebook (with prompts students can run and answer)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a4310e-1fe7-41ae-893d-f5c041b786a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
