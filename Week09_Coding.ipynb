{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86c7c92-f5da-4a89-9aa7-4f3e06f85be7",
   "metadata": {},
   "source": [
    "# Week 8: Applications of BERT\n",
    "\n",
    "In this notebook we explore practical ways to use BERT-style encoders:\n",
    "\n",
    "1. **Masked Language Modeling (MLM):** We will demonstrate two ways to use this option:\n",
    "    - **Single-word** masked prediction:** Predict the most likely token for `[MASK]` using context on both sides.\n",
    "    - **Multiple masks:** See why independent predictions aren’t coordinated, then try a left-to-right “coordinated fill” to produce coherent combinations.\n",
    "2. **Sentiment classification:** Fine-tune a compact BERT (DistilBERT) on IMDB movie reviews and visualize training/validation curves.\n",
    "3. **Relationship of sentences:** Score whether sentence **B** plausibly follows sentence **A** using BERT’s Next Sentence Prediction (NSP) head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b273c10a-aad2-43ab-8e21-749460bf3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# --- Standard Library ---\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# --- Third-Party Libraries ---\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- TensorFlow / Keras ---\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Sequential, layers, models\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay, ExponentialDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# Layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, GlobalAveragePooling1D, Dense, \n",
    "    LSTM, GRU, Dropout, SpatialDropout1D, Bidirectional, Lambda\n",
    ")\n",
    "\n",
    "# Preprocessing\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# --- Reproducibility Settings ---\n",
    "random_seed = 42\n",
    "\n",
    "'''\n",
    "# OS-level determinism\n",
    "os.environ['PYTHONHASHSEED'] = '0'        # Disable hash randomization\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'  # Make TF ops deterministic (where possible)\n",
    "os.environ['TF_CUDNN_DETERMINISM'] = '1'  # CuDNN deterministic (if using GPU)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow INFO and WARNING messages\n",
    "\n",
    "# Set seeds\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "'''\n",
    "\n",
    "# --- Utility Function ---\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n",
    "'''\n",
    "# Example usage for timing code:\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# <your code here>\n",
    "\n",
    "print(\"Execution Time:\", format_hms(time.time() - start_time))\n",
    "'''\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cc32a5-30f1-42fb-8c6b-8812d1bf0e44",
   "metadata": {},
   "source": [
    "### Logging and Displaying Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9910d75f-8eaf-4941-ba37-c5fdf6af4d52",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "def print_metrics():\n",
    "    # Print results sorted by accuracy (highest → lowest)\n",
    "\n",
    "    for title, (acc, ep) in sorted(results.items(), \n",
    "                                   key=lambda kv: kv[1][0],   # kv[1] is (acc, epoch); [0] is acc\n",
    "                                   reverse=True):\n",
    "        print(f\"{title:<40}\\t{acc:.4f} @ {ep}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c63ad8-8ccb-4d59-a859-e22c58b01e49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_learning_curves(hist, \n",
    "                         title=\"Learning Curves\", \n",
    "                         best_epoch=None,\n",
    "                         verbose=True,\n",
    "                         lr_logger=None       # NEW: pass LRSchedulerLogger callback\n",
    "                        ):\n",
    "    val_losses = hist.history['val_loss']\n",
    "    \n",
    "    # Determine best epoch (min val_loss or provided)\n",
    "    if best_epoch is not None:\n",
    "        min_val_epoch = best_epoch\n",
    "    else:\n",
    "        min_val_epoch = val_losses.index(min(val_losses))\n",
    "        \n",
    "    val_loss_at_min_loss = hist.history['val_loss'][min_val_epoch]    \n",
    "    val_acc_at_min_loss = hist.history['val_accuracy'][min_val_epoch]\n",
    "\n",
    "    n_epochs = len(val_losses)\n",
    "    epochs = list(range(1, n_epochs + 1))\n",
    "\n",
    "    # Tick interval for x-axis\n",
    "    tick_interval = max(1, n_epochs // 20)\n",
    "    xticks = list(range(0, n_epochs + 1, tick_interval))\n",
    "\n",
    "    # Figure layout: add 3rd subplot if LR is provided\n",
    "    n_subplots = 3 if lr_logger is not None else 2\n",
    "    plt.figure(figsize=(8, 10 if n_subplots == 3 else 8))\n",
    "\n",
    "    # --- Loss Plot ---\n",
    "    plt.subplot(n_subplots, 1, 1)\n",
    "    plt.plot(epochs, hist.history['loss'], label='Train Loss')\n",
    "    plt.plot(epochs, hist.history['val_loss'], label='Val Loss')\n",
    "    plt.scatter(min_val_epoch + 1, val_loss_at_min_loss, color='red', marker='x', s=50, label='Min Val Loss') \n",
    "    plt.title(f'{title} - Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xticks(xticks)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # --- Accuracy Plot ---\n",
    "    plt.subplot(n_subplots, 1, 2)\n",
    "    plt.plot(epochs, hist.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(epochs, hist.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.scatter(min_val_epoch + 1, val_acc_at_min_loss, color='red', marker='x', s=50, label='Acc @ Min Val Loss')\n",
    "    plt.title(f'{title} - Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(xticks)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # --- LR Plot (if provided) ---\n",
    "    if lr_logger is not None:\n",
    "        plt.subplot(n_subplots, 1, 3)\n",
    "        plt.plot(epochs, lr_logger.lrs, color='gray', label='Learning Rate')\n",
    "        plt.title(f'{title} - Learning Rate Schedule')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.xticks(xticks)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary\n",
    "    if verbose:\n",
    "        print(f\"Final Training Loss:            {hist.history['loss'][-1]:.4f}\")\n",
    "        print(f\"Final Training Accuracy:        {hist.history['accuracy'][-1]:.4f}\")\n",
    "        print(f\"Final Validation Loss:          {hist.history['val_loss'][-1]:.4f}\")\n",
    "        print(f\"Final Validation Accuracy:      {hist.history['val_accuracy'][-1]:.4f}\")\n",
    "        print(f\"Minimum Validation Loss:        {val_loss_at_min_loss:.4f} (Epoch {min_val_epoch + 1})\")\n",
    "        print(f\"Validation Accuracy @ Min Loss: {val_acc_at_min_loss:.4f}\")\n",
    "\n",
    "    # Store result in global results dict\n",
    "    results[title] = (val_acc_at_min_loss, min_val_epoch + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36530528-7474-4464-bc7e-f2e69054a606",
   "metadata": {},
   "source": [
    "### LR Schedulers and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c3182a-b2a3-42fb-ac29-f412963293a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Reduce the learning rate by half if val_loss does not improve for 5 epochs,\n",
    "# but never go below 1e-7\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',    # Quantity to be monitored.\n",
    "    factor=0.5,            # Factor by which the learning rate will be reduced.\n",
    "                           # new_lr = lr * factor\n",
    "    patience=5,            # Number of epochs with no improvement\n",
    "                           # after which learning rate will be reduced.\n",
    "    min_delta=1e-5,        # Threshold for measuring the new optimum,\n",
    "                           # to only focus on significant changes.\n",
    "    cooldown=0,            # Number of epochs to wait before resuming\n",
    "                           # normal operation after lr has been reduced.\n",
    "    min_lr=1e-8,           # Lower bound on the learning rate.\n",
    "    verbose=0,             # 0: quiet, 1: update messages.\n",
    ")\n",
    "\n",
    "# Copy this into cell where X_train, epochs, and batch_size are defined\n",
    "'''\n",
    "exp_decay = ExponentialDecay(\n",
    "    initial_learning_rate = 0.00001,  # ─ the starting learning rate (before any decay)\n",
    "    decay_steps = epochs * int(np.ceil(len(X_train) / batch_size)),  # ─ how many training steps (batches) before decay rate % is reached\n",
    "    decay_rate  = 0.9,                # ─ target % of lr after decay_steps steps (training of one batch)\n",
    "    staircase   = False,              # ─ if True, decay in discrete intervals (floor(step/decay_steps)),\n",
    "                                      #   if False, decay smoothly each step\n",
    ")\n",
    "'''\n",
    "\n",
    "# Ditto, copy this\n",
    "'''\n",
    "cosine_decay = CosineDecay(\n",
    "    initial_learning_rate=0.00001,   # ─ the starting learning rate\n",
    "    decay_steps = epochs * int(np.ceil(len(X_train) / batch_size)),            # ─ number of training steps (batches) over which to decay\n",
    "    alpha=0.0,                    # ─ minimum learning rate value as a fraction of initial_learning_rate\n",
    "                                  #    (final_lr = initial_lr * alpha)\n",
    ")\n",
    "'''\n",
    "\n",
    "# Used to display learning rate plot\n",
    "\n",
    "class LRSchedulerLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lrs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Get the optimizer's current LR (handles schedules automatically)\n",
    "        lr = self.model.optimizer.learning_rate\n",
    "        if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            current_step = tf.cast(self.model.optimizer.iterations, tf.float32)\n",
    "            lr = lr(current_step).numpy()\n",
    "        else:\n",
    "            lr = tf.keras.backend.get_value(lr)\n",
    "        self.lrs.append(lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663d90e9-85cc-4310-889d-be1627a43479",
   "metadata": {},
   "source": [
    "### Train and Test Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4792f578-c045-49bc-8cff-21a9dbfcdf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam    # tf.keras, not keras\n",
    "from tensorflow.keras.callbacks  import EarlyStopping, Callback\n",
    "\n",
    "# If you have a custom logger, make sure it subclasses tf.keras.callbacks.Callback\n",
    "class LRSchedulerLogger(Callback):\n",
    "    _supports_tf_logs = True\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.lrs = []\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        try:\n",
    "            lr = tf.keras.backend.get_value(self.model.optimizer.learning_rate)\n",
    "        except Exception:\n",
    "            lr = getattr(self.model.optimizer, \"learning_rate\", None)\n",
    "            if hasattr(lr, \"numpy\"): lr = lr.numpy()\n",
    "        self.lrs.append(float(lr) if lr is not None else None)\n",
    "\n",
    "def train_and_test(model, X_train, y_train, X_test, y_test,\n",
    "                   title         = \"Learning Curves\",\n",
    "                   epochs        = 200,\n",
    "                   lr_schedule   = 0.0001,\n",
    "                   optimizer     = \"Adam\",\n",
    "                   loss          = \"binary_crossentropy\",\n",
    "                   batch_size    = 256,\n",
    "                   use_early_stopping = True,\n",
    "                   patience      = 10,\n",
    "                   min_delta     = 1e-4,\n",
    "                   log_learning_rate = True,\n",
    "                   callbacks     = None,          # <- FIX: default None (no shared list)\n",
    "                   verbose       = 0,\n",
    "                   return_history = True\n",
    "                  ):\n",
    "    print(f\"{title}:  \", end=\"\")\n",
    "\n",
    "    # fresh callbacks list every call\n",
    "    callbacks = list(callbacks) if callbacks is not None else []\n",
    "\n",
    "    # Optimizer (ensure tf.keras)\n",
    "    if isinstance(optimizer, str):\n",
    "        if optimizer.lower() == \"adam\":\n",
    "            opt = Adam(learning_rate=lr_schedule)\n",
    "        else:\n",
    "            opt = optimizer\n",
    "    else:\n",
    "        opt = optimizer\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Install callbacks\n",
    "    if use_early_stopping:\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=patience, min_delta=min_delta,\n",
    "            restore_best_weights=True, verbose=verbose\n",
    "        )\n",
    "        callbacks.append(early_stop)\n",
    "\n",
    "    lr_logger = None\n",
    "    if log_learning_rate:\n",
    "        lr_logger = LRSchedulerLogger()\n",
    "        callbacks.append(lr_logger)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    best_epoch = int(np.argmin(history.history[\"val_loss\"]))\n",
    "    best_acc   = float(history.history[\"val_accuracy\"][best_epoch])\n",
    "\n",
    "    # Your existing plotting helper\n",
    "    plot_learning_curves(history, title=title, lr_logger=lr_logger)\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=verbose)\n",
    "\n",
    "    print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"\\nValidation-Test Gap (accuracy): {abs(best_acc - test_accuracy):.6f}\")\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"\\nExecution Time: {format_hms(end-start)}\")\n",
    "\n",
    "    if return_history:\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4d2e9a0-2b7b-47e2-93c3-7a250fc1c85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab/local once per runtime (safe to re-run)\n",
    "# !pip -q install \"tensorflow==2.17.0\" \"tf-keras==2.17.0\" \"transformers==4.44.2\"\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TORCH\"] = \"1\"   # keep backend on TF for consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44162393-cee0-4ce2-b099-9ba6def8141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3aca89-604d-4844-9fec-abdadd6123fc",
   "metadata": {},
   "source": [
    "## 1. Basic Masked Language Modeling with BERT\n",
    "\n",
    "We now demonstrate how a BERT-style encoder predicts missing words marked as `[MASK]`.\n",
    "\n",
    "**Notes:**\n",
    "- BERT is **bidirectional** — it looks at the words **before and after** the mask and picks the most likely token in that position.\n",
    "- **Why this matters:** MLM trains encoders to learn syntax (e.g., subject–verb agreement) and semantics (word sense from context), which makes them useful for classification, QA, etc. Note that it does **not** do any kind of reasoning; it simply calculates probabilities based on its training, much as your N-Gram language generation code did. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a91b44f-6abb-4c4a-afee-16d5eb7f8cb7",
   "metadata": {},
   "source": [
    "### Filling in a Single Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c01c3d13-e7b7-4f31-ad07-ee5f490ae015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The keys [MASK] on the table.\n",
      "\n",
      "        Word |  Probability\n",
      "---------------------------\n",
      "        were | 0.5268\n",
      "         lay | 0.1232\n",
      "         are | 0.0349\n",
      "\n",
      "The key  [MASK] on the table.\n",
      "\n",
      "        Word |  Probability\n",
      "---------------------------\n",
      "         was | 0.4977\n",
      "         lay | 0.1415\n",
      "      rested | 0.0636\n",
      "\n",
      "He went to the [MASK] to withdraw cash.\n",
      "\n",
      "        Word |  Probability\n",
      "---------------------------\n",
      "        bank | 0.9382\n",
      "       store | 0.0098\n",
      "         atm | 0.0073\n",
      "       banks | 0.0036\n",
      "      office | 0.0033\n",
      "\n",
      "The fish swam near the [MASK] of the river.\n",
      "\n",
      "        Word |  Probability\n",
      "---------------------------\n",
      "       mouth | 0.3574\n",
      "        edge | 0.1705\n",
      "        bank | 0.1014\n",
      "       banks | 0.0540\n",
      "      bottom | 0.0447\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "fill = pipeline(\"fill-mask\", model=\"bert-base-uncased\", framework=\"tf\")\n",
    "fill.tokenizer.clean_up_tokenization_spaces = True  # nicer printing\n",
    "\n",
    "def demo(s, k=3):\n",
    "    preds = fill(s, top_k=k)\n",
    "    print(\"\\n\" + s + \"\\n\")\n",
    "    print(f\"{'Word':>12} | {'Probability':>12}\")\n",
    "    print(\"-\" * 27)\n",
    "    for p in preds:\n",
    "        print(f\"{p['token_str'].strip():>12} | {p['score']:.4f}\")\n",
    "\n",
    "\n",
    "demo(\"The keys [MASK] on the table.\", 3)    \n",
    "demo(\"The key  [MASK] on the table.\", 3)   \n",
    "demo(\"He went to the [MASK] to withdraw cash.\", 5)  \n",
    "demo(\"The fish swam near the [MASK] of the river.\", 5) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797874f9-7ca3-4d69-aed3-d7be41d31ec6",
   "metadata": {},
   "source": [
    "#### We can also score a list of suggested fills:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dbfb5d0-7b4c-463e-86db-2e4195cf7378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat is a type of [MASK]. \n",
      "\n",
      "      Word |  Probability\n",
      "-------------------------\n",
      "    animal | 0.024314\n",
      "   vehicle | 0.000027\n",
      "     color | 0.000005\n"
     ]
    }
   ],
   "source": [
    "text = \"A cat is a type of [MASK].\"\n",
    "\n",
    "res = fill(text,\n",
    "           targets=[\"animal\", \"vehicle\", \"color\"])\n",
    "\n",
    "print(text,\"\\n\")\n",
    "print(f\"{'Word':>10} | {'Probability':>12}\")\n",
    "print(\"-\" * 25)\n",
    "for p in res:\n",
    "    print(f\"{p['token_str'].strip():>10} | {p['score']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e439c-1e45-46bf-b04a-86697412eecb",
   "metadata": {},
   "source": [
    "### Multiple Masks in One Sentence (Unlinked Predictions)\n",
    "\n",
    "> When a sentence contains more than one `[MASK]` token, the BERT *fill-mask* pipeline predicts each mask **independently**, keeping the others as `[MASK]`.\n",
    "> This means that the predictions for Mask 1, Mask 2, and Mask 3 are **not coordinated** — each list shows the most likely words *in that position alone*, not combinations that make a coherent sentence.\n",
    "\n",
    "So in the example below, the model gives separate lists of possibilities for each `[MASK]`, but they don’t necessarily form meaningful sequences together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5b03583-a804-4925-a0ae-a24c54049de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The [MASK] of The United [MASK] lives in the [MASK] House.\n",
      "Mask 1: ['president', 'embassy', 'queen', 'ambassador', 'government']\n",
      "Mask 2: ['kingdom', 'states', 'nations', 'nation', 'emirates']\n",
      "Mask 3: ['white', 'same', 'guest', 'opera', 'glass']\n"
     ]
    }
   ],
   "source": [
    "text = \"The [MASK] of The United [MASK] lives in the [MASK] House.\"\n",
    "pred_lists = fill(text, top_k=5)  # list of lists (one per mask)\n",
    "\n",
    "print(text)\n",
    "for i, plist in enumerate(pred_lists, 1):\n",
    "    words = [p[\"token_str\"].strip() for p in plist]\n",
    "    print(f\"Mask {i}: {words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2defb-d569-44e3-89d9-b53881b55fad",
   "metadata": {},
   "source": [
    "### Multiple Masks in One Sentence: Linked\n",
    "\n",
    "To generate **coherent replacements** across several `[MASK]` tokens, we can perform a **left-to-right coordinated fill** using a small *beam search*.\n",
    "\n",
    "This approach fills one mask at a time, using the best-scoring partial sentences to guide the next prediction.\n",
    "It works for Hugging Face pipeline outputs whether they return a **flat list** (one mask) or a **list of lists** (multiple masks), and it returns a ranked list of completed sentences with their associated probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b61b38d3-d879-4eb4-9bed-0a6d39be1a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The [MASK] of the United [MASK] lives in the [MASK] House. \n",
      "\n",
      "The president of the United states lives in the white House.   (p=0.54)\n",
      "The president of the United states lives in the state House.   (p=0.03)\n",
      "The president of the United states lives in the guest House.   (p=0.03)\n",
      "The president of the United states lives in the glass House.   (p=0.01)\n",
      "The president of the United states lives in the washington House.   (p=0.01)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def coordinated_fill(fill_pipeline, text, beam_size=5, per_mask_k=5):\n",
    "    MASK = fill_pipeline.tokenizer.mask_token\n",
    "    beams = [(text, 0.0)]  # (current_text, logprob)\n",
    "\n",
    "    while MASK in beams[0][0]:\n",
    "        new_beams = []\n",
    "        for s, lp in beams:\n",
    "            preds = fill_pipeline(s, top_k=per_mask_k)\n",
    "\n",
    "            # Normalize shapes: if multiple masks, HF may return list-of-lists; take first mask's list.\n",
    "            if isinstance(preds, list) and preds and isinstance(preds[0], list):\n",
    "                preds = preds[0]\n",
    "\n",
    "            for p in preds:\n",
    "                token = p[\"token_str\"].strip()\n",
    "                score = float(p[\"score\"])\n",
    "                # Replace only the FIRST [MASK]\n",
    "                s_next = s.replace(MASK, token, 1)\n",
    "                new_beams.append((s_next, lp + math.log(max(score, 1e-12))))\n",
    "\n",
    "        # Keep top `beam_size` hypotheses\n",
    "        new_beams.sort(key=lambda t: t[1], reverse=True)\n",
    "        beams = new_beams[:beam_size]\n",
    "\n",
    "    return beams\n",
    "\n",
    "# Example\n",
    "text = \"The [MASK] of the United [MASK] lives in the [MASK] House.\"\n",
    "\n",
    "print(text,\"\\n\")\n",
    "joint = coordinated_fill(fill, text, beam_size=5, per_mask_k=5)\n",
    "for s, lp in joint:\n",
    "    print(f\"{s}   (p={np.exp(lp):.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3e392-c9a8-4aa5-b09d-64960fa91638",
   "metadata": {},
   "source": [
    "## 2. Text Classification using BERT\n",
    "\n",
    "We return to the IMBD Movie Review dataset, to show how BERT can be used as a text classifier,  using a BERT model that’s already set up for classification:\n",
    "`TFAutoModelForSequenceClassification`. It’s the TensorFlow/Keras wrapper that adds a small classification head on top of BERT (dropout + dense layer on the [CLS] representation) and returns logits for your labels.\n",
    "\n",
    "We'll retrain BERT using our dataset, which unfreezes every layer, including the small classification head. \n",
    "\n",
    "Here are the steps we'll follow:\n",
    "\n",
    "1. Tokenize texts → tensors\n",
    "\n",
    "    - Convert raw reviews to input_ids and attention_mask with a BERT tokenizer.\n",
    "    \n",
    "    - Use truncation=True, a sensible max_length (128 is plenty for IMDB), and pad to that length.\n",
    "\n",
    "2. Model\n",
    "\n",
    "    - Load TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2).\n",
    "    \n",
    "    - This attaches the classification head; no manual head building needed.\n",
    "\n",
    "3. Loss/metrics\n",
    "\n",
    "    - Use SparseCategoricalCrossentropy(from_logits=True) (because the model returns logits).\n",
    "    \n",
    "    - Track accuracy. (You can add F1 later for reports.)\n",
    "\n",
    "4. Optimizer & LR schedule\n",
    "\n",
    "    - Use HF’s create_optimizer to get AdamW + warmup (good defaults for transformers).\n",
    "\n",
    "5. Train/eval loop\n",
    "\n",
    "    - Use plain Keras model.fit(...) (works with dict inputs).\n",
    "    \n",
    "    - Keep your existing train_and_test(...) plotting and early-stopping style.\n",
    "\n",
    "6. Inference\n",
    "\n",
    "    - argmax(logits, -1) → predicted class (0=NEG, 1=POS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da39509-f5b7-48dd-beb6-afde5725ad7c",
   "metadata": {},
   "source": [
    "### Tokenize IMDB → NumPy arrays (works with validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01d1b3da-7c26-420e-ba63-2fe4c0584745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "MODEL = \"distilbert-base-uncased\"\n",
    "MAX_LEN = 128\n",
    "batch_size = 256  # chunk size for tokenization to keep memory sane\n",
    "\n",
    "ds = load_dataset(\"imdb\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def to_str_list(x):\n",
    "    # Ensure we pass a plain Python list of strings to the tokenizer\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return list(x)\n",
    "    try:\n",
    "        return list(x)  # works for HF Arrow arrays, pandas Series, etc.\n",
    "    except TypeError:\n",
    "        # single string?\n",
    "        return [str(x)]\n",
    "\n",
    "def tokenize_to_np(texts, max_len=MAX_LEN, batch_size=batch_size):\n",
    "    texts = to_str_list(texts)\n",
    "    all_input_ids = []\n",
    "    all_attn = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        chunk = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            chunk,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"np\",\n",
    "        )\n",
    "        all_input_ids.append(enc[\"input_ids\"])\n",
    "        all_attn.append(enc[\"attention_mask\"])\n",
    "    input_ids = np.concatenate(all_input_ids, axis=0)\n",
    "    attention_mask = np.concatenate(all_attn, axis=0)\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "# Pull texts/labels (can subselect for speed while testing)\n",
    "train_texts = ds[\"train\"][\"text\"]\n",
    "train_labels = ds[\"train\"][\"label\"]\n",
    "test_texts  = ds[\"test\"][\"text\"]\n",
    "test_labels = ds[\"test\"][\"label\"]\n",
    "\n",
    "X_train = tokenize_to_np(train_texts)\n",
    "y_train = np.array(train_labels, dtype=\"int32\")\n",
    "\n",
    "X_test  = tokenize_to_np(test_texts)\n",
    "y_test  = np.array(test_labels, dtype=\"int32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0eae7-0717-4d53-a29b-be4934a63257",
   "metadata": {},
   "source": [
    "### Build a TF/Keras classifier (already a Keras `Model`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de4c2c68-291a-4414-8355-fac45678ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918feb4d-3d8b-4a9e-976e-7f07c48cf29e",
   "metadata": {},
   "source": [
    "### Train and Test\n",
    "\n",
    "Use a **logits**-aware loss; everything else can stay with your defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fc82bf-636b-4685-9e28-fa5caff4903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "# HF optimizer (tf.keras-compatible) – optional; you can also just pass optimizer=\"adam\"\n",
    "epochs = 5\n",
    "steps_per_epoch   = (int(0.8 * X_train[\"input_ids\"].shape[0]) // batch_size)\n",
    "total_train_steps = max(1, steps_per_epoch * epochs)\n",
    "warmup_steps      = int(0.1 * total_train_steps)\n",
    "\n",
    "optimizer, _ = create_optimizer(\n",
    "    init_lr=2e-5, num_warmup_steps=warmup_steps, num_train_steps=total_train_steps\n",
    ")\n",
    "\n",
    "hist = train_and_test(\n",
    "    model,\n",
    "    X_train, y_train,\n",
    "    X_test,  y_test,\n",
    "    title=\"IMDB — DistilBERT (TF/Keras)\",\n",
    "    epochs=epochs,\n",
    "    optimizer=optimizer,  # or \"adam\" and set lr_schedule=2e-5\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    batch_size=batch_size,\n",
    "    use_early_stopping=False,    # OFF for first run\n",
    "    log_learning_rate=False,     # OFF for first run\n",
    "    callbacks=[],                # ensure empty list\n",
    "    verbose=1,\n",
    "    return_history=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e20e4c4-f54f-499b-8f15-5d693a20e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "logits = model.predict(X_test, verbose=0).logits\n",
    "y_pred = logits.argmax(axis=-1)\n",
    "print(classification_report(y_test, y_pred, target_names=[\"NEG\",\"POS\"], digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c235e6-cb38-402f-aa8c-5df89d02af40",
   "metadata": {},
   "source": [
    "## 3. Classifying the Relationship of Sentences with BERT\n",
    "\n",
    "We use **BERT’s Next Sentence Prediction (NSP)** head to score whether a sentence **B** plausibly follows sentence **A**.\n",
    "\n",
    "- **Input:** a pair of sentences *(A, B)* tokenized together.\n",
    "- **Model:** `BertForNextSentencePrediction` (or TF: `TFBertForNextSentencePrediction`).\n",
    "  - Warning: `DistilBERT` does **not** have an NSP head—use full BERT (e.g., `bert-base-uncased`).\n",
    "- **Output:** two logits for classes **[IS_NEXT, NOT_NEXT]**; we apply softmax to get  \n",
    "  $ p(\\text{IS\\_NEXT}\\mid A,B) $ and $ p(\\text{NOT\\_NEXT}\\mid A,B) $.\n",
    "\n",
    "**How to interpret:**  \n",
    "- Higher $ p(\\text{IS\\_NEXT}) $ means B is a plausible continuation of A (per BERT’s pretraining).  \n",
    "- Scores are **not perfectly calibrated**; NSP tends to be overconfident for **topically related** pairs.\n",
    "\n",
    "**Good demo patterns:**\n",
    "- Compare the true continuation vs. a same-topic-but-wrong sentence vs. an off-topic sentence, and **rank by $ p(\\text{IS\\_NEXT}) $**.\n",
    "- For clearer behavior, construct **hard negatives** (shuffle within the same paragraph) rather than random sentences.\n",
    "\n",
    "**When to fine-tune instead:**  \n",
    "If you need “nextness” that matches your domain or dataset, fine-tune a small classifier on your own consecutive vs. mismatched pairs rather than relying on the pretrained NSP head.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4bc75df-5e1c-44ff-a138-f183dda21f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Make sure both resources are present\n",
    "for pkg, handle in [\n",
    "    (\"punkt\", \"tokenizers/punkt\"),\n",
    "    (\"punkt_tab\", \"tokenizers/punkt_tab\"),\n",
    "]:\n",
    "    try:\n",
    "        nltk.data.find(handle)\n",
    "    except LookupError:\n",
    "        nltk.download(pkg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98d746f6-f203-4f9f-a077-fcaef266f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install transformers torch\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForNextSentencePrediction\n",
    "\n",
    "# BERT checkpoints include an NSP head; DistilBERT does NOT.\n",
    "MODEL = \"bert-base-uncased\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL)\n",
    "nsp = AutoModelForNextSentencePrediction.from_pretrained(MODEL)\n",
    "nsp.eval()  # inference mode\n",
    "\n",
    "def next_sentence_prob(s1: str, s2: str):\n",
    "    \"\"\"\n",
    "    Returns: (p_is_next, p_not_next, predicted_label)\n",
    "    \"\"\"\n",
    "    inputs = tok(s1, s2, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "    with torch.no_grad():\n",
    "        logits = nsp(**inputs).logits  # [batch=1, 2] -> [IsNext, NotNext]\n",
    "        probs = torch.softmax(logits, dim=-1).squeeze(0).tolist()\n",
    "    p_is_next, p_not_next = probs[0], probs[1]\n",
    "    label = \"IS_NEXT\" if p_is_next >= p_not_next else \"NOT_NEXT\"\n",
    "    return p_is_next, p_not_next, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d19175ad-c6af-497c-b233-2a6fe00b2816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Natural language processing models are widely used in industry.\n",
      "\n",
      "Candidate                                                                        | p(IS_NEXT) | Label\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Boosting methods like XGBoost focus on hard-to-predict examples.                 |   0.999970 | IS_NEXT\n",
      "They power search, chatbots, and content moderation.                             |   0.999966 | IS_NEXT\n",
      "But the future of AI is uncertain.                                               |   0.999946 | IS_NEXT\n",
      "And most students want to learn them for this reason.                            |   0.999928 | IS_NEXT\n",
      "The coal industry, however, is in decline and does not use these techniques.     |   0.998985 | IS_NEXT\n",
      "But the President tweeted something about AI, so the future of NLP in industry i |   0.992282 | IS_NEXT\n",
      "So the class is quite popular.                                                   |   0.035544 | NOT_NEXT\n",
      "Most industries are affected.                                                    |   0.024545 | NOT_NEXT\n",
      "Most industries are affected by this trend.                                      |   0.011500 | NOT_NEXT\n",
      "The coal industry, however, does not.                                            |   0.000503 | NOT_NEXT\n",
      "But the President tweeted something, so the future of NLP in industry is unclear |   0.000138 | NOT_NEXT\n",
      "The Eiffel Tower is located in Paris, France.                                    |   0.000008 | NOT_NEXT\n",
      "The President announced a new round of tariffs.                                  |   0.000004 | NOT_NEXT\n"
     ]
    }
   ],
   "source": [
    "def show_candidates(s1, s2_list):\n",
    "    rows = []\n",
    "    for s2 in s2_list:\n",
    "        p_next, p_not, lab = next_sentence_prob(s1, s2)\n",
    "        rows.append((s2, p_next, lab))\n",
    "    rows.sort(key=lambda r: r[1], reverse=True)\n",
    "    print(f\"Context: {s1}\\n\")\n",
    "    print(f\"{'Candidate':<80} | {'p(IS_NEXT)':>10} | Label\")\n",
    "    print(\"-\"*110)\n",
    "    for s2, p, lab in rows:\n",
    "        print(f\"{s2[:80]:<80} | {p:10.6f} | {lab}\")\n",
    "\n",
    "s1 = \"Natural language processing models are widely used in industry.\"\n",
    "cands = [\n",
    "    \"They power search, chatbots, and content moderation.\",             # true next\n",
    "    \"Boosting methods like XGBoost focus on hard-to-predict examples.\", # same topic\n",
    "    \"The Eiffel Tower is located in Paris, France.\",                    # off-topic\n",
    "    \"The President announced a new round of tariffs.\",\n",
    "    \"But the President tweeted something about AI, so the future of NLP in industry is unclear.\",\n",
    "    \"But the President tweeted something, so the future of NLP in industry is unclear.\",\n",
    "    \"So the class is quite popular.\",\n",
    "    \"The coal industry, however, is in decline and does not use these techniques.\",\n",
    "    \"The coal industry, however, does not.\",\n",
    "    \"Most industries are affected.\",\n",
    "    \"Most industries are affected by this trend.\",\n",
    "    \"And most students want to learn them for this reason.\",\n",
    "    \"But the future of AI is uncertain.\"\n",
    "]\n",
    "show_candidates(s1, cands)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
